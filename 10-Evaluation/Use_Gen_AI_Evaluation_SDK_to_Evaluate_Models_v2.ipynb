{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNc5B1-pOvFn"
      },
      "source": [
        "# Use Gen AI Evaluation SDK to Evaluate Models in Vertex AI Studio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_7WkHE3gNO"
      },
      "source": [
        "This notebook demonstrates how to get started with using the *Vertex AI Python SDK for Gen AI Evaluation Service* for generative models in Vertex AI Studio, Model Garden, and Model Registry.\n",
        "\n",
        "Gen AI Evaluation Service empowers you to comprehensively assess and enhance your generative AI models and applications. Whether you're selecting the ideal model, optimizing prompt templates, or evaluating fine-tuned checkpoints, this service provides the tools and insights you need.\n",
        "\n",
        "In this Colab tutorial, we'll explore three major use cases:\n",
        "\n",
        "1.  Run Evaluation on 1P Models\n",
        "  *   Learn how to evaluate `Gemini` models in Vertex AI Studio using the *Gen AI Evaluation Service SDK*.\n",
        "\n",
        "  *   Explore different evaluation metrics and techniques for assessing performance on various tasks.\n",
        "\n",
        "  *   Discover how to leverage the SDK for in-depth analysis and comparison of `Gemini` model variants.\n",
        "2.  Prompt Engineering\n",
        "\n",
        "  *   Explore the impact of prompt design on model performance.\n",
        "  *   Utilize the SDK to systematically evaluate and refine your prompts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHGZmbkw6GgM"
      },
      "source": [
        "For additional use cases and advanced features, refer to our public documentation and notebook tutorials for evaluation use cases:\n",
        "\n",
        "* https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview#notebooks_for_evaluation_use_cases\n",
        "\n",
        "* https://cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN5IHo-aOvFo"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XZf_4VEOvFo"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE20na1OOvFo"
      },
      "source": [
        "### Install Vertex AI SDK for Gen AI Evaluation Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abLuRgBzOvFp"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-cloud-aiplatform[evaluation] openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oLkh17OvFp"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ygOCeYoOvFp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "  from google.colab import auth\n",
        "\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyNclIAOvFp"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTL_YzF9OvFq"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"project-ee7cb7bb-2d31-45e1-83d\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "  raise ValueError(\"Please set your PROJECT_ID\")\n",
        "\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBQgjn5wOvFq"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TmyCxUSOvFq"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXkKKzk8OvFq"
      },
      "source": [
        "## Load an evaluation dataset\n",
        "\n",
        "Load a subset of the `OpenOrca` dataset using the `huggingface/datasets` library. We will use 10 samples from the first 100 rows of the \"train\" split of `OpenOrca` dataset to demonstrate evaluating prompts and model responses in this Colab.\n",
        "\n",
        "### Dataset Summary\n",
        "\n",
        "The OpenOrca dataset is a collection of augmented [FLAN Collection data](https://arxiv.org/abs/2301.13688). Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper and currently represents a partial completion of the full intended dataset, with ongoing generation to expand its scope. The data is primarily used for training and evaluation in the field of natural language processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-j7l4Qd0Ull"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from google.auth import default, transport\n",
        "from vertexai.evaluation import (\n",
        "    EvalTask,\n",
        "    MetricPromptTemplateExamples,\n",
        "    PairwiseMetric,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.preview.evaluation import notebook_utils\n",
        "\n",
        "ds = (\n",
        "    load_dataset(\n",
        "        \"Open-Orca/OpenOrca\",\n",
        "        data_files=\"1M-GPT4-Augmented.parquet\",\n",
        "        split=\"train[:100]\",\n",
        "    )\n",
        "    .to_pandas()\n",
        "    .drop(columns=[\"id\"])\n",
        "    .rename(columns={\"response\": \"reference\"})\n",
        ")\n",
        "\n",
        "dataset = ds.sample(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOeVlflg06F3"
      },
      "source": [
        "#### Preview the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAHGlmkEelhm"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgrS1Ljo0VVQ"
      },
      "source": [
        "## Run evaluation on Gemini models\n",
        "\n",
        "The *Gen AI Evaluation Service SDK* includes native support for evaluating Gemini models. This streamlines the evaluation process for Google's latest and most capable family of large language models. With minimal coding effort, you can leverage pre-defined metrics and workflows to assess the performance of Gemini models on various tasks. You can also customize your own model-based metrics based on your specific evaluation criteria.\n",
        "\n",
        "This enhanced support enables you to:\n",
        "\n",
        "- **Quickly evaluate Gemini models:** Effortlessly assess the performance of Gemini models using the SDK's streamlined workflows.\n",
        "- **Compare models side-by-side:** Benchmark Gemini against other models to understand relative strengths and weaknesses.\n",
        "- **Analyze prompt templates:** Evaluate the effectiveness of different prompt designs for optimizing Gemini's performance.\n",
        "\n",
        "This native integration simplifies the evaluation process, allowing you to focus on understanding and improving the capabilities of Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011966ad51c4"
      },
      "source": [
        "#### Understand the `EvalTask` class\n",
        "\n",
        "The `EvalTask` class is a core component of the *Gen AI Evaluation Service SDK* framework. It allows you to define and run evaluation jobs against your Gen AI models/applications, providing a structured way to measure their performance on specific tasks. Think of an `EvalTask` as a blueprint for your evaluation process.\n",
        "\n",
        "`EvalTask` class requires an evaluation dataset and a list of metrics. Supported metrics are documented on the Generative AI on Vertex AI [Define your evaluation metrics](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) page. The dataset can be an `pandas.DataFrame`, Python dictionary or a file path URI and can contain default column names such as `prompt`, `reference`, `response`, and `baseline_model_response`.\n",
        "\n",
        "* **Bring-your-own-response (BYOR):** You already have the data that you want to evaluate stored in the dataset. You can customize the response column names for both your model and the baseline model using parameters like `response_column_name` and `baseline_model_response_column_name` or through the `metric_column_mapping`.\n",
        "\n",
        "* **Perform model inference without a prompt template:** You have a dataset containing the input prompts to the model and want to perform model inference before evaluation. A column named `prompt` is required in the evaluation dataset and is used directly as input to the model.\n",
        "\n",
        "* **Perform model inference with a prompt template:** You have a dataset containing the input variables to the prompt template and want to assemble the prompts for model inference. Evaluation dataset must contain column names corresponding to the variable names in the prompt template. For example, if prompt template is \"Instruction: {instruction}, context: {context}\", the dataset must contain `instruction` and `context` columns.\n",
        "\n",
        "\n",
        "\n",
        "`EvalTask` supports extensive evaluation scenarios including BYOR, model inference with Gemini models, 3P models endpoints/SDK clients, or custom model generation functions, using computation-based metrics, model-based pointwise and pairwise metrics. The `evaluate()` method triggers the evaluation process, optionally taking a model, prompt template, experiment logging configurations, and other evaluation run configurations. You can view the SDK reference documentation for [Gen AI Evaluation package](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.evaluation) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHGa6rD6-ks"
      },
      "source": [
        "### Define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol1fa0hI7Irr"
      },
      "outputs": [],
      "source": [
        "# Model to be evaluated\n",
        "model = GenerativeModel(\n",
        "    \"gemini-2.0-flash\",\n",
        "    generation_config={\n",
        "        \"temperature\": 0.6,\n",
        "        \"max_output_tokens\": 256,\n",
        "        \"top_k\": 1,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuvdzrUIayPp"
      },
      "source": [
        "### Use computation-based metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrY-NLGg0TVS"
      },
      "outputs": [],
      "source": [
        "# Define an EvalTask with ROUGE-L-SUM metric\n",
        "rouge_eval_task = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[\"rouge_l_sum\"],\n",
        ")\n",
        "rouge_result = rouge_eval_task.evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSMRnDMixwLS"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_eval_result(rouge_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx9pKjiSbHag"
      },
      "source": [
        "### Use model-based pointwise metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuRlYbj0HvBB"
      },
      "outputs": [],
      "source": [
        "# @title Select a pointwise metric to use\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "pointwise_single_turn_metrics = [\n",
        "    metric\n",
        "    for metric in MetricPromptTemplateExamples.list_example_metric_names()\n",
        "    if not metric.startswith(\"pairwise\") and not metric.startswith(\"multi_turn\")\n",
        "]\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=pointwise_single_turn_metrics,\n",
        "    description=\"Select a metric:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "  global POINTWISE_METRIC\n",
        "  if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "    POINTWISE_METRIC = change.new\n",
        "    print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "POINTWISE_METRIC = dropdown.value\n",
        "dropdown.observe(dropdown_eventhandler, names=\"value\")\n",
        "display(dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBnyzogabOmp"
      },
      "outputs": [],
      "source": [
        "pointwise_result = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[POINTWISE_METRIC],\n",
        ").evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EwLaPusyeL9"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_eval_result(pointwise_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsPD-YUWoRYo"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_explanations(\n",
        "    pointwise_result, num=1, metrics=[POINTWISE_METRIC]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AzW8Zg71Igs"
      },
      "source": [
        "#### Build your own pointwise metric\n",
        "\n",
        "For more information about metric customization, see this [notebook tutorial](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/customize_model_based_metrics.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MJdLUJz6H5F"
      },
      "outputs": [],
      "source": [
        "pointwise_result = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        # Create a unique model-based metric for your own use cases\n",
        "        PointwiseMetric(\n",
        "            metric=\"linguistic_acceptability\",\n",
        "            metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "                criteria={\n",
        "                    \"Proper Grammar\": (\n",
        "                        \"The language's grammar rules are correctly followed,\"\n",
        "                        \" including but not limited to sentence structures,\"\n",
        "                        \" verb tenses, subject-verb agreement, proper\"\n",
        "                        \" punctuation, and capitalization.\"\n",
        "                    ),\n",
        "                    \"Appropriate word choice\": (\n",
        "                        \"Words chosen are appropriate and purposeful given\"\n",
        "                        \" their relative context and positioning in the text.\"\n",
        "                        \" Vocabulary demonstrates prompt understanding.\"\n",
        "                    ),\n",
        "                    \"Reference Alignment\": (\n",
        "                        \"The response is consistent and aligned with the\"\n",
        "                        \" reference.\"\n",
        "                    ),\n",
        "                },\n",
        "                rating_rubric={\n",
        "                    \"5\": (\n",
        "                        \"Excellent: The writing is grammatically correct, uses\"\n",
        "                        \" appropriate vocabulary and aligns perfectly with the\"\n",
        "                        \" reference.\"\n",
        "                    ),\n",
        "                    \"4\": (\n",
        "                        \"Good: The writing is generally grammatically correct,\"\n",
        "                        \" uses appropriate vocabulary and aligns well with the\"\n",
        "                        \" reference.\"\n",
        "                    ),\n",
        "                    \"3\": (\n",
        "                        \"Satisfactory: The writing may have minor grammatical\"\n",
        "                        \" errors or use less-appropriate vocabulary, but it\"\n",
        "                        \" aligns reasonably well with the reference.\"\n",
        "                    ),\n",
        "                    \"2\": (\n",
        "                        \"Unsatisfactory: The writing has significant\"\n",
        "                        \" grammatical errors, uses inappropriate vocabulary,\"\n",
        "                        \" deviates significantly from the reference.\"\n",
        "                    ),\n",
        "                    \"1\": (\n",
        "                        \"Poor: The writing is riddled with grammatical errors,\"\n",
        "                        \" uses highly inappropriate vocabulary, is completely\"\n",
        "                        \" unrelated to the reference.\"\n",
        "                    ),\n",
        "                },\n",
        "                input_variables=[\"prompt\", \"reference\"],\n",
        "            ),\n",
        "        )\n",
        "    ],\n",
        ").evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW5DXIGv7oIJ"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_eval_result(pointwise_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHRuWjv86uG"
      },
      "source": [
        "### Use model-based pairwise metrics\n",
        "\n",
        "Evaluate two Gen AI models side-by-side (SxS) with model-based pairwise metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Eqh9e9MDsa"
      },
      "outputs": [],
      "source": [
        "# @title Select a pairwise metric to use\n",
        "\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "pairwise_single_turn_metrics = [\n",
        "    metric\n",
        "    for metric in MetricPromptTemplateExamples.list_example_metric_names()\n",
        "    if metric.startswith(\"pairwise\") and \"multi_turn\" not in metric\n",
        "]\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=pairwise_single_turn_metrics,\n",
        "    description=\"Select a metric:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "  global PAIRWISE_METRIC_NAME\n",
        "  if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "    PAIRWISE_METRIC_NAME = change.new\n",
        "    print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "PAIRWISE_METRIC_NAME = dropdown.value\n",
        "dropdown.observe(dropdown_eventhandler, names=\"value\")\n",
        "display(dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJd6sF7re5gC"
      },
      "source": [
        "#### Run SxS evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDAaYDqY9FBu"
      },
      "outputs": [],
      "source": [
        "pairwise_result = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        PairwiseMetric(\n",
        "            metric=PAIRWISE_METRIC_NAME,\n",
        "            metric_prompt_template=MetricPromptTemplateExamples.get_prompt_template(\n",
        "                PAIRWISE_METRIC_NAME\n",
        "            ),\n",
        "            # Define a baseline model to compare against\n",
        "            baseline_model=GenerativeModel(\"gemini-2.0-flash-lite\"),\n",
        "        )\n",
        "    ],\n",
        ").evaluate(\n",
        "    # Specify a candidate model for pairwise comparison\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPAoJu53-mfx"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_eval_result(\n",
        "    pairwise_result,\n",
        "    title=(\n",
        "        \"Gemini-2.0-Flash vs. Gemini-2.0-Flash-Lite SxS Pairwise Evaluation\"\n",
        "        \" Results\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLHuqe2D-x81"
      },
      "source": [
        "## Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd393d80e252"
      },
      "source": [
        "The *Vertex AI Gen AI Evaluation Service SDK* simplifies prompt engineering by streamlining the process of creating and evaluating multiple prompt templates. It allows you to efficiently test different prompts against a chosen dataset and compare their performance using comprehensive evaluation metrics. This empowers you to identify the most effective prompts for your specific use case and optimize your generative AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5261f69b615a"
      },
      "source": [
        "###  Compare and optimize prompt template design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d01f70c3163"
      },
      "source": [
        "#### Define an evaluation dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2157353f3309"
      },
      "source": [
        "To perform pointwise inference, the evaluation dataset is required to contain the following fields:\n",
        "\n",
        "* Instruction: Part of the input user prompt. It refers to the inference instruction that is sent to your LLM.\n",
        "* Context: User input for the Gen AI model or application in the current turn.\n",
        "* Reference: The ground truth to compare your LLM response to.\n",
        "\n",
        "Your dataset must include a minimum of one evaluation example. We recommend around 100 examples to ensure high-quality aggregated metrics and statistically significant results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "972ebabb2712"
      },
      "outputs": [],
      "source": [
        "instruction = \"Summarize the following article: \\n\"\n",
        "\n",
        "context = [\n",
        "    (\n",
        "        \"Typhoon Phanfone has killed at least one person, a US airman on\"\n",
        "        \" Okinawa who was washed away by high waves. Thousands of households\"\n",
        "        \" have lost power and Japan's two largest airlines have suspended many\"\n",
        "        \" flights. The storm also forced the suspension of the search for\"\n",
        "        \" people missing after last week's volcanic eruption. The\"\n",
        "        \" storm-tracking website Tropical Storm Risk forecasts that Phanfone\"\n",
        "        \" will rapidly lose power over the next few hours as it goes further\"\n",
        "        \" into the Pacific Ocean. Typhoon Phanfone was downgraded from an\"\n",
        "        \" earlier status of a super typhoon, but the Japan Meteorological\"\n",
        "        \" Agency had warned it was still a dangerous storm. Japan averages 11\"\n",
        "        \" typhoons a year, according to its weather agency. The typhoon made\"\n",
        "        \" landfall on Monday morning near the central city of Hamamatsu, with\"\n",
        "        \" winds of up to 180 km/h (112 mph). The airman was one of three US\"\n",
        "        \" military personnel swept away by high waves whipped up by the typhoon\"\n",
        "        \" off southern Okinawa island, where the US has a large military base.\"\n",
        "        \" The remaining two are still missing. A police spokesman said they had\"\n",
        "        \" been taking photographs of the sea. A university student who was\"\n",
        "        \" surfing off the seas of Kanagawa Prefecture, south of Tokyo, was also\"\n",
        "        \" missing, national broadcast NHK reports. It said at least 10 people\"\n",
        "        \" had been injured and 9,500 houses were without power. The storm was\"\n",
        "        \" expected to deposit about 100mm of rain on Tokyo over 24 hours,\"\n",
        "        \" according to the Transport Ministry website. Many schools were closed\"\n",
        "        \" on Monday and two car companies in Japan halted production at some\"\n",
        "        \" plants ahead of the storm. More than 174 domestic flights were\"\n",
        "        \" affected nationwide, NHK state broadcaster said on Sunday. On Sunday,\"\n",
        "        \" heavy rain delayed the Japanese Formula One Grand Prix in Suzaka.\"\n",
        "        \" French driver Jules Bianchi lost control in the wet conditions and\"\n",
        "        \" crashed, sustaining a severe head injury.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The blaze started at the detached building in Drivers End in Codicote,\"\n",
        "        \" near Welwyn, during the morning. There was another fire at the\"\n",
        "        \" building 20 years ago, after which fire-proof foil was placed under\"\n",
        "        \" the thatch, which is protecting the main building. More than 15 fire\"\n",
        "        \" engines and support vehicles were called to tackle the blaze. Roads\"\n",
        "        \" in the area were closed and traffic diverted.\"\n",
        "    ),\n",
        "    (\n",
        "        \"The 18-year-old fell at the New Charter Academy on Broadoak Road in\"\n",
        "        \" Ashton-under-Lyne at about 09:10 BST, Greater Manchester Police (GMP)\"\n",
        "        \" said. GMP said he had gone to Manchester Royal Infirmary and his\"\n",
        "        ' condition was \"serious\". Principal Jenny Langley said the school'\n",
        "        ' would remain \"fully open\" while police investigated. \"Our thoughts'\n",
        "        \" are with the family and we're doing everything we can to support\"\n",
        "        ' them along with staff and pupils,\" she said.'\n",
        "    ),\n",
        "    (\n",
        "        \"But Belgian-born Dutchman Max Verstappen was unable to drive a car\"\n",
        "        \" legally on his own in either country. That all changed on Wednesday\"\n",
        "        \" when the youngster turned 18 and passed his driving test at the first\"\n",
        "        \" attempt. Despite having competed in 14 grands prix since his debut in\"\n",
        "        \" Australia in March, Verstappen admitted to feeling the pressure\"\n",
        "        ' during his test. \"It\\'s a relief,\" said the Toro Rosso driver, who'\n",
        "        \" finished ninth in Japan on Sunday and had only started driving\"\n",
        "        ' lessons a week ago. \"I was a bit nervous to make mistakes, but the'\n",
        "        ' exam went well.\" A bonus of turning 18 is that Verstappen will now be'\n",
        "        \" able to drink the champagne if he ever makes it onto the podium.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "reference = [\n",
        "    (\n",
        "        \"A powerful typhoon has brought many parts of Japan to a standstill and\"\n",
        "        \" briefly battered Tokyo before heading out to sea.\"\n",
        "    ),\n",
        "    (\n",
        "        \"A major fire has been burning in the thatched roof of a large property\"\n",
        "        \" in Hertfordshire.\"\n",
        "    ),\n",
        "    (\n",
        "        \"A student has been taken to hospital after falling from a balcony at a\"\n",
        "        \" Greater Manchester school.\"\n",
        "    ),\n",
        "    (\n",
        "        \"He is Formula 1's youngest ever driver and in charge of a car that can\"\n",
        "        \" reach over 200mph.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = [\n",
        "    (\n",
        "        \"Typhoon Phanfone, while downgraded from super typhoon status, caused\"\n",
        "        \" significant disruption and tragedy in Japan. One US airman died after\"\n",
        "        \" being swept away by high waves, with two more missing. The storm\"\n",
        "        \" caused power outages for thousands, flight cancellations, and the\"\n",
        "        \" suspension of rescue efforts for missing volcano victims. Heavy rain\"\n",
        "        \" and strong winds led to school and factory closures, transportation\"\n",
        "        \" disruptions, and at least 10 injuries. The typhoon is expected to\"\n",
        "        \" weaken as it moves over the Pacific Ocean.\"\n",
        "    ),\n",
        "    (\n",
        "        \"A large fire broke out in a detached thatched building in Codicote,\"\n",
        "        \" near Welwyn. This is the second fire at the building in 20 years.\"\n",
        "        \" Thankfully, fire-proof foil installed after the previous fire is\"\n",
        "        \" protecting the main building. Over 15 fire engines and support\"\n",
        "        \" vehicles responded, closing roads and diverting traffic in the area.\"\n",
        "    ),\n",
        "    (\n",
        "        \"An 18-year-old student at New Charter Academy in Ashton-under-Lyne\"\n",
        "        \" suffered a serious fall and was hospitalized. The incident is under\"\n",
        "        \" investigation by Greater Manchester Police, but the school remains\"\n",
        "        \" open. The principal expressed support for the student's family and\"\n",
        "        \" the school community.\"\n",
        "    ),\n",
        "    (\n",
        "        \"Max Verstappen, a Formula One driver, was finally able to get his\"\n",
        "        \" driver's license at age 18. Despite already competing in 14 Grand\"\n",
        "        \" Prix races, he was not of legal driving age in his native countries.\"\n",
        "        \" He admitted to being nervous but passed the test on his first\"\n",
        "        \" attempt.  As an added bonus of turning 18, Verstappen can now enjoy\"\n",
        "        \" champagne on the podium if he places.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "eval_dataset = pd.DataFrame({\n",
        "    \"instruction\": instruction,\n",
        "    \"context\": context,\n",
        "    \"reference\": reference,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3c1121684f5"
      },
      "source": [
        "#### Define an EvalTask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "052ec86e5777"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"eval-sdk-prompt-engineering\"  # @param {type:\"string\"}\n",
        "\n",
        "summarization_eval_task = EvalTask(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=[\n",
        "        \"rouge_l_sum\",\n",
        "        \"bleu\",\n",
        "        \"fluency\",\n",
        "        \"coherence\",\n",
        "        \"safety\",\n",
        "        \"groundedness\",\n",
        "        \"summarization_quality\",\n",
        "        \"verbosity\",\n",
        "        \"instruction_following\",\n",
        "        \"text_quality\",\n",
        "    ],\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515fe8c3652f"
      },
      "source": [
        "#### Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b8422213915"
      },
      "outputs": [],
      "source": [
        "# Define prompt templates to compare\n",
        "prompt_templates = [\n",
        "    (\n",
        "        \"Instruction: {instruction} such that you're explaining it to a 5 year\"\n",
        "        \" old. Article: {context}. Summary:\"\n",
        "    ),\n",
        "    \"Article: {context}. Complete this task: {instruction}. Summary:\",\n",
        "    (\n",
        "        \"Goal: {instruction} and give me a TL;DR in five words. Here's an\"\n",
        "        \" article: {context}. Summary:\"\n",
        "    ),\n",
        "    (\n",
        "        \"Article: {context}. Reference Summary: {reference}. {instruction} to\"\n",
        "        \" be more concise and verbose than the reference.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "eval_results = []\n",
        "for i, prompt_template in enumerate(prompt_templates):\n",
        "  eval_result = summarization_eval_task.evaluate(\n",
        "      prompt_template=prompt_template,\n",
        "      model=GenerativeModel(\n",
        "          \"gemini-2.0-flash\",\n",
        "          generation_config={\n",
        "              \"temperature\": 0.3,\n",
        "              \"max_output_tokens\": 256,\n",
        "              \"top_k\": 1,\n",
        "          },\n",
        "      ),\n",
        "      # Customize eval service rate limit to improve speed.\n",
        "      # See more details in https://cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation#increase-quota\n",
        "      evaluation_service_qps=5,\n",
        "  )\n",
        "\n",
        "  eval_results.append((f\"Prompt Template #{i + 1}\", eval_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec6e97cf9f27"
      },
      "source": [
        "#### Display Evaluation report and explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3724b416b39"
      },
      "outputs": [],
      "source": [
        "for title, eval_result in eval_results:\n",
        "  notebook_utils.display_eval_result(title=title, eval_result=eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0615d0925469"
      },
      "outputs": [],
      "source": [
        "for title, eval_result in eval_results:\n",
        "  notebook_utils.display_explanations(\n",
        "      eval_result, metrics=[\"summarization_quality\"], num=2\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzWSUPj2oV-_"
      },
      "source": [
        "#### Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSDyxE2aoYMH"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_radar_plot(\n",
        "    eval_results,\n",
        "    metrics=[\"instruction_following\", \"fluency\", \"coherence\", \"text_quality\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id5sjYHboZHh"
      },
      "outputs": [],
      "source": [
        "notebook_utils.display_bar_plot(\n",
        "    eval_results,\n",
        "    metrics=[\"instruction_following\", \"fluency\", \"coherence\", \"text_quality\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed966819648e"
      },
      "source": [
        "####  View Experiment log for evaluation runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "363c1b2553b9"
      },
      "outputs": [],
      "source": [
        "summarization_eval_task.display_runs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7cce28cc97e"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef73672573e1"
      },
      "outputs": [],
      "source": [
        "delete_experiment = True\n",
        "\n",
        "# Please set your LOCATION to the same one used during Vertex AI SDK initialization.\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "if delete_experiment:\n",
        "  from google.cloud import aiplatform\n",
        "\n",
        "  aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "  experiment = aiplatform.Experiment(EXPERIMENT_NAME)\n",
        "  experiment.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "name": "Use_Gen_AI_Evaluation_SDK_to_Evaluate_Models_v1.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
